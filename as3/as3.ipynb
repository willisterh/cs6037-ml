{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Will Hollingsworth, Colton Murray, Alexander Shiveley"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting the data into Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the csv as a numpy array of strings, \n",
    "# because it includes the column headers\n",
    "raw_data = np.loadtxt('iris.csv', delimiter=',', dtype=str)\n",
    "\n",
    "# Grab the headers\n",
    "header_list = raw_data[0].tolist()\n",
    "\n",
    "# Remove the headers from the data\n",
    "stripped = np.delete(raw_data, [0], axis=0)\n",
    "\n",
    "# We specifically want to know if a sample is setosa or NOT setosa\n",
    "def apply_mapping(row):\n",
    "    row[4] = 1 if row[4] == 'setosa' else 0\n",
    "    return row\n",
    "\n",
    "# Convert species to a numeric value\n",
    "converted = np.apply_along_axis(apply_mapping, 1, stripped)\n",
    "\n",
    "# Convert everything into floats!\n",
    "clean_data = np.array(converted, dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.1, 3.5, 1.4, 0.2, 1. ],\n",
       "       [4.9, 3. , 1.4, 0.2, 1. ],\n",
       "       [4.7, 3.2, 1.3, 0.2, 1. ],\n",
       "       [4.6, 3.1, 1.5, 0.2, 1. ],\n",
       "       [5. , 3.6, 1.4, 0.2, 1. ],\n",
       "       [5.4, 3.9, 1.7, 0.4, 1. ],\n",
       "       [4.6, 3.4, 1.4, 0.3, 1. ],\n",
       "       [5. , 3.4, 1.5, 0.2, 1. ],\n",
       "       [4.4, 2.9, 1.4, 0.2, 1. ],\n",
       "       [4.9, 3.1, 1.5, 0.1, 1. ],\n",
       "       [5.4, 3.7, 1.5, 0.2, 1. ],\n",
       "       [4.8, 3.4, 1.6, 0.2, 1. ],\n",
       "       [4.8, 3. , 1.4, 0.1, 1. ],\n",
       "       [4.3, 3. , 1.1, 0.1, 1. ],\n",
       "       [5.8, 4. , 1.2, 0.2, 1. ],\n",
       "       [5.7, 4.4, 1.5, 0.4, 1. ],\n",
       "       [5.4, 3.9, 1.3, 0.4, 1. ],\n",
       "       [5.1, 3.5, 1.4, 0.3, 1. ],\n",
       "       [5.7, 3.8, 1.7, 0.3, 1. ],\n",
       "       [5.1, 3.8, 1.5, 0.3, 1. ],\n",
       "       [5.4, 3.4, 1.7, 0.2, 1. ],\n",
       "       [5.1, 3.7, 1.5, 0.4, 1. ],\n",
       "       [4.6, 3.6, 1. , 0.2, 1. ],\n",
       "       [5.1, 3.3, 1.7, 0.5, 1. ],\n",
       "       [4.8, 3.4, 1.9, 0.2, 1. ],\n",
       "       [5. , 3. , 1.6, 0.2, 1. ],\n",
       "       [5. , 3.4, 1.6, 0.4, 1. ],\n",
       "       [5.2, 3.5, 1.5, 0.2, 1. ],\n",
       "       [5.2, 3.4, 1.4, 0.2, 1. ],\n",
       "       [4.7, 3.2, 1.6, 0.2, 1. ],\n",
       "       [4.8, 3.1, 1.6, 0.2, 1. ],\n",
       "       [5.4, 3.4, 1.5, 0.4, 1. ],\n",
       "       [5.2, 4.1, 1.5, 0.1, 1. ],\n",
       "       [5.5, 4.2, 1.4, 0.2, 1. ],\n",
       "       [4.9, 3.1, 1.5, 0.2, 1. ],\n",
       "       [5. , 3.2, 1.2, 0.2, 1. ],\n",
       "       [5.5, 3.5, 1.3, 0.2, 1. ],\n",
       "       [4.9, 3.6, 1.4, 0.1, 1. ],\n",
       "       [4.4, 3. , 1.3, 0.2, 1. ],\n",
       "       [5.1, 3.4, 1.5, 0.2, 1. ],\n",
       "       [5. , 3.5, 1.3, 0.3, 1. ],\n",
       "       [4.5, 2.3, 1.3, 0.3, 1. ],\n",
       "       [4.4, 3.2, 1.3, 0.2, 1. ],\n",
       "       [5. , 3.5, 1.6, 0.6, 1. ],\n",
       "       [5.1, 3.8, 1.9, 0.4, 1. ],\n",
       "       [4.8, 3. , 1.4, 0.3, 1. ],\n",
       "       [5.1, 3.8, 1.6, 0.2, 1. ],\n",
       "       [4.6, 3.2, 1.4, 0.2, 1. ],\n",
       "       [5.3, 3.7, 1.5, 0.2, 1. ],\n",
       "       [5. , 3.3, 1.4, 0.2, 1. ],\n",
       "       [7. , 3.2, 4.7, 1.4, 0. ],\n",
       "       [6.4, 3.2, 4.5, 1.5, 0. ],\n",
       "       [6.9, 3.1, 4.9, 1.5, 0. ],\n",
       "       [5.5, 2.3, 4. , 1.3, 0. ],\n",
       "       [6.5, 2.8, 4.6, 1.5, 0. ],\n",
       "       [5.7, 2.8, 4.5, 1.3, 0. ],\n",
       "       [6.3, 3.3, 4.7, 1.6, 0. ],\n",
       "       [4.9, 2.4, 3.3, 1. , 0. ],\n",
       "       [6.6, 2.9, 4.6, 1.3, 0. ],\n",
       "       [5.2, 2.7, 3.9, 1.4, 0. ],\n",
       "       [5. , 2. , 3.5, 1. , 0. ],\n",
       "       [5.9, 3. , 4.2, 1.5, 0. ],\n",
       "       [6. , 2.2, 4. , 1. , 0. ],\n",
       "       [6.1, 2.9, 4.7, 1.4, 0. ],\n",
       "       [5.6, 2.9, 3.6, 1.3, 0. ],\n",
       "       [6.7, 3.1, 4.4, 1.4, 0. ],\n",
       "       [5.6, 3. , 4.5, 1.5, 0. ],\n",
       "       [5.8, 2.7, 4.1, 1. , 0. ],\n",
       "       [6.2, 2.2, 4.5, 1.5, 0. ],\n",
       "       [5.6, 2.5, 3.9, 1.1, 0. ],\n",
       "       [5.9, 3.2, 4.8, 1.8, 0. ],\n",
       "       [6.1, 2.8, 4. , 1.3, 0. ],\n",
       "       [6.3, 2.5, 4.9, 1.5, 0. ],\n",
       "       [6.1, 2.8, 4.7, 1.2, 0. ],\n",
       "       [6.4, 2.9, 4.3, 1.3, 0. ],\n",
       "       [6.6, 3. , 4.4, 1.4, 0. ],\n",
       "       [6.8, 2.8, 4.8, 1.4, 0. ],\n",
       "       [6.7, 3. , 5. , 1.7, 0. ],\n",
       "       [6. , 2.9, 4.5, 1.5, 0. ],\n",
       "       [5.7, 2.6, 3.5, 1. , 0. ],\n",
       "       [5.5, 2.4, 3.8, 1.1, 0. ],\n",
       "       [5.5, 2.4, 3.7, 1. , 0. ],\n",
       "       [5.8, 2.7, 3.9, 1.2, 0. ],\n",
       "       [6. , 2.7, 5.1, 1.6, 0. ],\n",
       "       [5.4, 3. , 4.5, 1.5, 0. ],\n",
       "       [6. , 3.4, 4.5, 1.6, 0. ],\n",
       "       [6.7, 3.1, 4.7, 1.5, 0. ],\n",
       "       [6.3, 2.3, 4.4, 1.3, 0. ],\n",
       "       [5.6, 3. , 4.1, 1.3, 0. ],\n",
       "       [5.5, 2.5, 4. , 1.3, 0. ],\n",
       "       [5.5, 2.6, 4.4, 1.2, 0. ],\n",
       "       [6.1, 3. , 4.6, 1.4, 0. ],\n",
       "       [5.8, 2.6, 4. , 1.2, 0. ],\n",
       "       [5. , 2.3, 3.3, 1. , 0. ],\n",
       "       [5.6, 2.7, 4.2, 1.3, 0. ],\n",
       "       [5.7, 3. , 4.2, 1.2, 0. ],\n",
       "       [5.7, 2.9, 4.2, 1.3, 0. ],\n",
       "       [6.2, 2.9, 4.3, 1.3, 0. ],\n",
       "       [5.1, 2.5, 3. , 1.1, 0. ],\n",
       "       [5.7, 2.8, 4.1, 1.3, 0. ],\n",
       "       [6.3, 3.3, 6. , 2.5, 0. ],\n",
       "       [5.8, 2.7, 5.1, 1.9, 0. ],\n",
       "       [7.1, 3. , 5.9, 2.1, 0. ],\n",
       "       [6.3, 2.9, 5.6, 1.8, 0. ],\n",
       "       [6.5, 3. , 5.8, 2.2, 0. ],\n",
       "       [7.6, 3. , 6.6, 2.1, 0. ],\n",
       "       [4.9, 2.5, 4.5, 1.7, 0. ],\n",
       "       [7.3, 2.9, 6.3, 1.8, 0. ],\n",
       "       [6.7, 2.5, 5.8, 1.8, 0. ],\n",
       "       [7.2, 3.6, 6.1, 2.5, 0. ],\n",
       "       [6.5, 3.2, 5.1, 2. , 0. ],\n",
       "       [6.4, 2.7, 5.3, 1.9, 0. ],\n",
       "       [6.8, 3. , 5.5, 2.1, 0. ],\n",
       "       [5.7, 2.5, 5. , 2. , 0. ],\n",
       "       [5.8, 2.8, 5.1, 2.4, 0. ],\n",
       "       [6.4, 3.2, 5.3, 2.3, 0. ],\n",
       "       [6.5, 3. , 5.5, 1.8, 0. ],\n",
       "       [7.7, 3.8, 6.7, 2.2, 0. ],\n",
       "       [7.7, 2.6, 6.9, 2.3, 0. ],\n",
       "       [6. , 2.2, 5. , 1.5, 0. ],\n",
       "       [6.9, 3.2, 5.7, 2.3, 0. ],\n",
       "       [5.6, 2.8, 4.9, 2. , 0. ],\n",
       "       [7.7, 2.8, 6.7, 2. , 0. ],\n",
       "       [6.3, 2.7, 4.9, 1.8, 0. ],\n",
       "       [6.7, 3.3, 5.7, 2.1, 0. ],\n",
       "       [7.2, 3.2, 6. , 1.8, 0. ],\n",
       "       [6.2, 2.8, 4.8, 1.8, 0. ],\n",
       "       [6.1, 3. , 4.9, 1.8, 0. ],\n",
       "       [6.4, 2.8, 5.6, 2.1, 0. ],\n",
       "       [7.2, 3. , 5.8, 1.6, 0. ],\n",
       "       [7.4, 2.8, 6.1, 1.9, 0. ],\n",
       "       [7.9, 3.8, 6.4, 2. , 0. ],\n",
       "       [6.4, 2.8, 5.6, 2.2, 0. ],\n",
       "       [6.3, 2.8, 5.1, 1.5, 0. ],\n",
       "       [6.1, 2.6, 5.6, 1.4, 0. ],\n",
       "       [7.7, 3. , 6.1, 2.3, 0. ],\n",
       "       [6.3, 3.4, 5.6, 2.4, 0. ],\n",
       "       [6.4, 3.1, 5.5, 1.8, 0. ],\n",
       "       [6. , 3. , 4.8, 1.8, 0. ],\n",
       "       [6.9, 3.1, 5.4, 2.1, 0. ],\n",
       "       [6.7, 3.1, 5.6, 2.4, 0. ],\n",
       "       [6.9, 3.1, 5.1, 2.3, 0. ],\n",
       "       [5.8, 2.7, 5.1, 1.9, 0. ],\n",
       "       [6.8, 3.2, 5.9, 2.3, 0. ],\n",
       "       [6.7, 3.3, 5.7, 2.5, 0. ],\n",
       "       [6.7, 3. , 5.2, 2.3, 0. ],\n",
       "       [6.3, 2.5, 5. , 1.9, 0. ],\n",
       "       [6.5, 3. , 5.2, 2. , 0. ],\n",
       "       [6.2, 3.4, 5.4, 2.3, 0. ],\n",
       "       [5.9, 3. , 5.1, 1.8, 0. ]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_input_output(data):\n",
    "    \"\"\"\n",
    "    Splits the input array into two seperate sets:\n",
    "        * the feature values\n",
    "        * the output value\n",
    "        \n",
    "    :returns: (tuple) the features are the first element, the outputs are the second\n",
    "    \"\"\"\n",
    "    return np.delete(data, [data.shape[1] - 1], axis=1), data[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[5.1, 3.5, 1.4, 0.2],\n",
       "        [4.9, 3. , 1.4, 0.2],\n",
       "        [4.7, 3.2, 1.3, 0.2],\n",
       "        [4.6, 3.1, 1.5, 0.2],\n",
       "        [5. , 3.6, 1.4, 0.2],\n",
       "        [5.4, 3.9, 1.7, 0.4],\n",
       "        [4.6, 3.4, 1.4, 0.3],\n",
       "        [5. , 3.4, 1.5, 0.2],\n",
       "        [4.4, 2.9, 1.4, 0.2],\n",
       "        [4.9, 3.1, 1.5, 0.1],\n",
       "        [5.4, 3.7, 1.5, 0.2],\n",
       "        [4.8, 3.4, 1.6, 0.2],\n",
       "        [4.8, 3. , 1.4, 0.1],\n",
       "        [4.3, 3. , 1.1, 0.1],\n",
       "        [5.8, 4. , 1.2, 0.2],\n",
       "        [5.7, 4.4, 1.5, 0.4],\n",
       "        [5.4, 3.9, 1.3, 0.4],\n",
       "        [5.1, 3.5, 1.4, 0.3],\n",
       "        [5.7, 3.8, 1.7, 0.3],\n",
       "        [5.1, 3.8, 1.5, 0.3],\n",
       "        [5.4, 3.4, 1.7, 0.2],\n",
       "        [5.1, 3.7, 1.5, 0.4],\n",
       "        [4.6, 3.6, 1. , 0.2],\n",
       "        [5.1, 3.3, 1.7, 0.5],\n",
       "        [4.8, 3.4, 1.9, 0.2],\n",
       "        [5. , 3. , 1.6, 0.2],\n",
       "        [5. , 3.4, 1.6, 0.4],\n",
       "        [5.2, 3.5, 1.5, 0.2],\n",
       "        [5.2, 3.4, 1.4, 0.2],\n",
       "        [4.7, 3.2, 1.6, 0.2],\n",
       "        [4.8, 3.1, 1.6, 0.2],\n",
       "        [5.4, 3.4, 1.5, 0.4],\n",
       "        [5.2, 4.1, 1.5, 0.1],\n",
       "        [5.5, 4.2, 1.4, 0.2],\n",
       "        [4.9, 3.1, 1.5, 0.2],\n",
       "        [5. , 3.2, 1.2, 0.2],\n",
       "        [5.5, 3.5, 1.3, 0.2],\n",
       "        [4.9, 3.6, 1.4, 0.1],\n",
       "        [4.4, 3. , 1.3, 0.2],\n",
       "        [5.1, 3.4, 1.5, 0.2],\n",
       "        [5. , 3.5, 1.3, 0.3],\n",
       "        [4.5, 2.3, 1.3, 0.3],\n",
       "        [4.4, 3.2, 1.3, 0.2],\n",
       "        [5. , 3.5, 1.6, 0.6],\n",
       "        [5.1, 3.8, 1.9, 0.4],\n",
       "        [4.8, 3. , 1.4, 0.3],\n",
       "        [5.1, 3.8, 1.6, 0.2],\n",
       "        [4.6, 3.2, 1.4, 0.2],\n",
       "        [5.3, 3.7, 1.5, 0.2],\n",
       "        [5. , 3.3, 1.4, 0.2],\n",
       "        [7. , 3.2, 4.7, 1.4],\n",
       "        [6.4, 3.2, 4.5, 1.5],\n",
       "        [6.9, 3.1, 4.9, 1.5],\n",
       "        [5.5, 2.3, 4. , 1.3],\n",
       "        [6.5, 2.8, 4.6, 1.5],\n",
       "        [5.7, 2.8, 4.5, 1.3],\n",
       "        [6.3, 3.3, 4.7, 1.6],\n",
       "        [4.9, 2.4, 3.3, 1. ],\n",
       "        [6.6, 2.9, 4.6, 1.3],\n",
       "        [5.2, 2.7, 3.9, 1.4],\n",
       "        [5. , 2. , 3.5, 1. ],\n",
       "        [5.9, 3. , 4.2, 1.5],\n",
       "        [6. , 2.2, 4. , 1. ],\n",
       "        [6.1, 2.9, 4.7, 1.4],\n",
       "        [5.6, 2.9, 3.6, 1.3],\n",
       "        [6.7, 3.1, 4.4, 1.4],\n",
       "        [5.6, 3. , 4.5, 1.5],\n",
       "        [5.8, 2.7, 4.1, 1. ],\n",
       "        [6.2, 2.2, 4.5, 1.5],\n",
       "        [5.6, 2.5, 3.9, 1.1],\n",
       "        [5.9, 3.2, 4.8, 1.8],\n",
       "        [6.1, 2.8, 4. , 1.3],\n",
       "        [6.3, 2.5, 4.9, 1.5],\n",
       "        [6.1, 2.8, 4.7, 1.2],\n",
       "        [6.4, 2.9, 4.3, 1.3],\n",
       "        [6.6, 3. , 4.4, 1.4],\n",
       "        [6.8, 2.8, 4.8, 1.4],\n",
       "        [6.7, 3. , 5. , 1.7],\n",
       "        [6. , 2.9, 4.5, 1.5],\n",
       "        [5.7, 2.6, 3.5, 1. ],\n",
       "        [5.5, 2.4, 3.8, 1.1],\n",
       "        [5.5, 2.4, 3.7, 1. ],\n",
       "        [5.8, 2.7, 3.9, 1.2],\n",
       "        [6. , 2.7, 5.1, 1.6],\n",
       "        [5.4, 3. , 4.5, 1.5],\n",
       "        [6. , 3.4, 4.5, 1.6],\n",
       "        [6.7, 3.1, 4.7, 1.5],\n",
       "        [6.3, 2.3, 4.4, 1.3],\n",
       "        [5.6, 3. , 4.1, 1.3],\n",
       "        [5.5, 2.5, 4. , 1.3],\n",
       "        [5.5, 2.6, 4.4, 1.2],\n",
       "        [6.1, 3. , 4.6, 1.4],\n",
       "        [5.8, 2.6, 4. , 1.2],\n",
       "        [5. , 2.3, 3.3, 1. ],\n",
       "        [5.6, 2.7, 4.2, 1.3],\n",
       "        [5.7, 3. , 4.2, 1.2],\n",
       "        [5.7, 2.9, 4.2, 1.3],\n",
       "        [6.2, 2.9, 4.3, 1.3],\n",
       "        [5.1, 2.5, 3. , 1.1],\n",
       "        [5.7, 2.8, 4.1, 1.3],\n",
       "        [6.3, 3.3, 6. , 2.5],\n",
       "        [5.8, 2.7, 5.1, 1.9],\n",
       "        [7.1, 3. , 5.9, 2.1],\n",
       "        [6.3, 2.9, 5.6, 1.8],\n",
       "        [6.5, 3. , 5.8, 2.2],\n",
       "        [7.6, 3. , 6.6, 2.1],\n",
       "        [4.9, 2.5, 4.5, 1.7],\n",
       "        [7.3, 2.9, 6.3, 1.8],\n",
       "        [6.7, 2.5, 5.8, 1.8],\n",
       "        [7.2, 3.6, 6.1, 2.5],\n",
       "        [6.5, 3.2, 5.1, 2. ],\n",
       "        [6.4, 2.7, 5.3, 1.9],\n",
       "        [6.8, 3. , 5.5, 2.1],\n",
       "        [5.7, 2.5, 5. , 2. ],\n",
       "        [5.8, 2.8, 5.1, 2.4],\n",
       "        [6.4, 3.2, 5.3, 2.3],\n",
       "        [6.5, 3. , 5.5, 1.8],\n",
       "        [7.7, 3.8, 6.7, 2.2],\n",
       "        [7.7, 2.6, 6.9, 2.3],\n",
       "        [6. , 2.2, 5. , 1.5],\n",
       "        [6.9, 3.2, 5.7, 2.3],\n",
       "        [5.6, 2.8, 4.9, 2. ],\n",
       "        [7.7, 2.8, 6.7, 2. ],\n",
       "        [6.3, 2.7, 4.9, 1.8],\n",
       "        [6.7, 3.3, 5.7, 2.1],\n",
       "        [7.2, 3.2, 6. , 1.8],\n",
       "        [6.2, 2.8, 4.8, 1.8],\n",
       "        [6.1, 3. , 4.9, 1.8],\n",
       "        [6.4, 2.8, 5.6, 2.1],\n",
       "        [7.2, 3. , 5.8, 1.6],\n",
       "        [7.4, 2.8, 6.1, 1.9],\n",
       "        [7.9, 3.8, 6.4, 2. ],\n",
       "        [6.4, 2.8, 5.6, 2.2],\n",
       "        [6.3, 2.8, 5.1, 1.5],\n",
       "        [6.1, 2.6, 5.6, 1.4],\n",
       "        [7.7, 3. , 6.1, 2.3],\n",
       "        [6.3, 3.4, 5.6, 2.4],\n",
       "        [6.4, 3.1, 5.5, 1.8],\n",
       "        [6. , 3. , 4.8, 1.8],\n",
       "        [6.9, 3.1, 5.4, 2.1],\n",
       "        [6.7, 3.1, 5.6, 2.4],\n",
       "        [6.9, 3.1, 5.1, 2.3],\n",
       "        [5.8, 2.7, 5.1, 1.9],\n",
       "        [6.8, 3.2, 5.9, 2.3],\n",
       "        [6.7, 3.3, 5.7, 2.5],\n",
       "        [6.7, 3. , 5.2, 2.3],\n",
       "        [6.3, 2.5, 5. , 1.9],\n",
       "        [6.5, 3. , 5.2, 2. ],\n",
       "        [6.2, 3.4, 5.4, 2.3],\n",
       "        [5.9, 3. , 5.1, 1.8]]),\n",
       " array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y = split_input_output(clean_data)\n",
    "x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we also probably want to be able to separate our positive and negative examples\n",
    "def split_pos_neg(data):\n",
    "    \"\"\"\n",
    "    Returns two sets of positive, then negative examples (1's then 0's from the output column)\n",
    "    \"\"\"\n",
    "    return data[data[:, -1]==1, :-1], data[data[:, -1]==0, :-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos, neg = split_pos_neg(clean_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_range(data, attribute):\n",
    "    return data[:, attribute].min(), data[:, attribute].max()\n",
    "\n",
    "def get_bin_edges(data, attribute, num_bins):\n",
    "    l, h = get_range(data, attribute)\n",
    "    \n",
    "    step = (h - l) / num_bins\n",
    "    edges = np.arange(num_bins + 1)\n",
    "    return (edges * step) + l\n",
    "\n",
    "def hist(data, attribute, num_bins):\n",
    "    \"\"\"\n",
    "    Produces two histograms, one for positive and one for negative examples.\n",
    "    Calculates the bins from the entire data set.\n",
    "    \n",
    "    :param data: The TOTAL data set\n",
    "    :attribute: the attribute (column index) you'd like to histogram\n",
    "    :num_bins: the number of bins for the histogram\n",
    "    \"\"\"\n",
    "    edges = get_bin_edges(data, attribute, num_bins)\n",
    "    pos, neg = split_pos_neg(data)\n",
    "    \n",
    "    h_pos, _ = np.histogram(pos[:, attribute], edges)\n",
    "    h_neg, _ = np.histogram(neg[:, attribute], edges)\n",
    "    \n",
    "    return h_pos, h_neg, edges\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([50,  0,  0,  0,  0], dtype=int64),\n",
       " array([ 0,  3, 34, 47, 16], dtype=int64),\n",
       " array([1.  , 2.18, 3.36, 4.54, 5.72, 6.9 ]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_pos, h_neg, e = hist(clean_data, 2, 5)\n",
    "h_pos, h_neg, e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ^ Notice how they're completely separate for petal length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_entropy(data_set):\n",
    "    \"\"\"\n",
    "    Calculates the entropy of a data set.\n",
    "    Assumes that the final column is the classification for each sample.\n",
    "    \"\"\"\n",
    "    x, y = split_input_output(data_set)\n",
    "    \n",
    "    # get the possible output values, and their totals\n",
    "    values, counts = np.unique(y, return_counts=True)\n",
    "    entropy = 0\n",
    "    for count in counts:\n",
    "        p = count / sum(counts)   \n",
    "        entropy -= p * math.log(p, 2)\n",
    "        # Alex - I know the base should be len(values), but in most cases this is 2 for this assignment.\n",
    "        # It caused issues for me in ID3 if len(values) was 1\n",
    "        \n",
    "    return entropy\n",
    "\n",
    "def bucket_entropy(pos_count, neg_count):\n",
    "    if pos_count == 0 or neg_count == 0:\n",
    "        return 0\n",
    "    \n",
    "    p = pos_count / (pos_count + neg_count)\n",
    "    return -p * math.log(p, 2)\n",
    "\n",
    "\n",
    "def info_gained(data_set, attribute, num_bins):\n",
    "    h_pos, h_neg, _ = hist(data_set, attribute, num_bins)\n",
    "    \n",
    "    tot_entropy = total_entropy(data_set)\n",
    "    for i in range(len(h_pos)):\n",
    "        bin_size_ratio = (h_pos[i] + h_neg[i]) / data_set.shape[0]\n",
    "        tot_entropy -= bin_size_ratio * bucket_entropy(h_pos[i], h_neg[i])\n",
    "        \n",
    "    return tot_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9182958340544896"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_entropy(clean_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9182958340544896"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info_gained(clean_data, 2, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ^ In this specific case, we gained ALL the info because the samples are perfectly seperated by petal length\n",
    "# Theoretically, we would check every attribute and choose whichever one yielded the highest gain\n",
    "# Then split our data_set by the bins of the histogram, if a bin has all the same kind of output we can stop there,\n",
    "# but if there are still bins with both kinds of output we repeat this info_gained on a new attribute for that bin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: make sure we're doing the bins right? the homework mentions rounding to the nearest integer, \n",
    "# currently we're just evenly splitting across the range of the attribute\n",
    "# Alex - I'm not sure. This doesn't make sense since one attribute ranges from about 0 to 3,\n",
    "# and rounding to ints won't work too well\n",
    "\n",
    "# TODO: actually form the tree? I don't think it's actually necessary but usually you would actually output the tree with\n",
    "# the sequences of tests it performs on which attributes\n",
    "# Alex - I went ahead and generated a tree in order to run the test data against.\n",
    "# I agree that it's not needed, but might be nice as output for the assignment. And it's not much extra work.\n",
    "\n",
    "# TODO: accuracy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ID3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "def in_range(value, val_range):\n",
    "    return value >= val_range[0] and value < val_range[1]\n",
    "\n",
    "def id3_filter_data(data_set, attribute, val_range):\n",
    "    rows = []\n",
    "    for row in range(data_set.shape[0]):\n",
    "            if in_range(data_set[row, attribute], val_range):\n",
    "                rows.append(row)\n",
    "    return data_set[rows, :]\n",
    "\n",
    "def id3_generate_tree(data_set, num_bins, decision_tree=None, used_attr=None):\n",
    "    \"\"\"\n",
    "    Recursively produces an ID3 tree based on the data set and number of bins.\n",
    "    \n",
    "    :param data_set: The TOTAL data set for this node or entire tree\n",
    "    :num_bins: the number of bins for the histogram\n",
    "    :decision_tree: the subtree to expand\n",
    "    :used_attr: list of used attributes so far down this branch\n",
    "    \"\"\"     \n",
    "    # Start with a root node if tree is empty\n",
    "    if decision_tree is None:\n",
    "        decision_tree = {\"attribute\": None, \"range\": None, \"children\": [], \"split\": None}\n",
    "    # Start with empty list of used attributes\n",
    "    if used_attr is None:\n",
    "        used_attr = []    \n",
    "    \n",
    "    # Find the attribute with the highest information gain, if the attribute is unused\n",
    "    best_attr = None\n",
    "    for attr in range(0, data_set.shape[1] - 1):\n",
    "        if not attr in used_attr:\n",
    "            if best_attr is None or info_gained(data_set, attr, num_bins) > info_gained(data_set, best_attr, num_bins):\n",
    "                best_attr = attr\n",
    "    if best_attr is not None:\n",
    "        decision_tree[\"attribute\"] = best_attr\n",
    "        used_attr.append(best_attr)\n",
    "    \n",
    "    # Get the histograms for the best attribute and the edges\n",
    "    h_pos, h_neg, e = hist(data_set, best_attr, num_bins)\n",
    "    # Add a node to the tree for each bin\n",
    "    for i in range(len(h_pos)):  \n",
    "        child = {}\n",
    "        # The range of the best attribute for this bin. Allow for bounds to infinities on ends of histogram\n",
    "        val_range = (e[i] if i > 0 else float('-inf'), e[i + 1] if i < len(h_pos) - 1 else float('inf'))\n",
    "        \n",
    "        # If the bin is all positive, all negative, empty?, or there are no unused attributes left, create a leaf node\n",
    "        if h_pos[i] > 0 and h_neg[i] == 0 or h_pos[i] == 0 and h_neg[i] > 0 or h_pos[i] == 0 or best_attr == None:\n",
    "            split = (h_pos[i], h_neg[i])\n",
    "            child = {\"attribute\": None, \"range\": val_range, \"children\": [], \"split\": split}      \n",
    "        # Else expand the bin using the filtered dataset for that bin    \n",
    "        else:\n",
    "            child = id3_generate_tree(id3_filter_data(data_set, best_attr, val_range), num_bins, {\"attribute\": None, \"range\": val_range, \"children\": [], \"split\": None}, used_attr.copy())\n",
    "        \n",
    "        decision_tree[\"children\"].append(child)\n",
    "    return decision_tree\n",
    "\n",
    "def id3_range_str(val_range):\n",
    "    \"\"\"\n",
    "    Print the range, accounting for infinities\n",
    "    \"\"\"\n",
    "    if val_range[0] == float('-inf'):\n",
    "        return \"<  \" + str(val_range[1])\n",
    "    if val_range[1] == float('inf'):\n",
    "        return \">= \" + str(val_range[0])\n",
    "    return \"in [\" + str(val_range[0]) + \", \" + str(val_range[1]) + \")\"\n",
    "\n",
    "def id3_print_tree(tree, depth = 0):\n",
    "    for child in tree[\"children\"]:        \n",
    "        print(\"    \" * depth, \"Attribute\", tree[\"attribute\"], id3_range_str(child[\"range\"]), \": \" + str(child[\"split\"]) if child[\"split\"] != None else \"\")\n",
    "        print_id3_tree(child, depth + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "def id3_classify(x_data_set, decision_tree):\n",
    "    \"\"\"\n",
    "    Classify y values for the data set using a decision tree\n",
    "    \n",
    "    :param x_data_set: The x part of the data set\n",
    "    :decision_tree: the tree to classify each point\n",
    "    \"\"\"     \n",
    "    y_matrix = np.empty((x_data_set.shape[0], 1))\n",
    "    for row in range(x_data_set.shape[0]):\n",
    "        node = decision_tree\n",
    "        while node[\"split\"] is None:\n",
    "            for child in node[\"children\"]:   \n",
    "                if in_range(x_data_set[row, node[\"attribute\"]], child[\"range\"]):\n",
    "                    node = child\n",
    "                    break\n",
    "        y_matrix[row, 0] = 1 if node[\"split\"][0] > node[\"split\"][1] else 0\n",
    "    return y_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Attribute 2 <  3.95 \n",
      "     Attribute 3 <  0.7499999999999999 : (50, 0)\n",
      "     Attribute 3 >= 0.7499999999999999 : (0, 11)\n",
      " Attribute 2 >= 3.95 : (0, 89)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.]])"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id3_tree = id3_generate_tree(clean_data, 2)\n",
    "print_id3_tree(id3_tree)\n",
    "id3_classify(clean_data, id3_tree) #TODO: Replace clean_data with test data once splitting of training/test data is done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And I'm not sure what to do with ID3 if a bin is empty. Right now it just has it as a leaf node and will classify as negative\n",
    "# if the bin is empty and a test data point happens to fall in that bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
