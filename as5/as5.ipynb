{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Will Hollingsworth, Colton Murray, Alexander Shiveley"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting the data into Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the csv as a numpy array of strings, \n",
    "# because it includes the column headers\n",
    "raw_data = np.loadtxt('data_banknote_authentication.txt', delimiter=',', dtype=str)\n",
    "\n",
    "# Update output class values of 0 to -1\n",
    "def apply_mapping(row):\n",
    "    row[4] = -1 if row[4] == 0 else 1\n",
    "    return row\n",
    "converted = np.apply_along_axis(apply_mapping, 1, raw_data)\n",
    "\n",
    "# Convert everything into floats!\n",
    "clean_data = np.array(converted, dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sets(data, split):\n",
    "    \"\"\"\n",
    "    Convenience function that randomly selects a training and test set from the input data.\n",
    "    \n",
    "    :param data: (ndarray) the data you want to split\n",
    "    :param split: (float array) the percentages of the data you want to be TRAINING, VALIDATION, and TESTING data\n",
    "    \n",
    "    :returns: (tuple) a tuple where the first element is the training set, and the second element is the test set\n",
    "    \"\"\"\n",
    "    # Randomly shuffle the order from a copy of the data\n",
    "    shuffled = data.copy()\n",
    "    np.random.shuffle(shuffled)\n",
    "\n",
    "    row_count = data.shape[0]\n",
    "\n",
    "    # calc the number of samples, assumes the input samples are seperated by row\n",
    "    training_count = round(row_count * split[0])\n",
    "    \n",
    "    training_set = shuffled[:training_count]\n",
    "    remaining_set = shuffled[training_count:]\n",
    "    \n",
    "    # calc the number of samples, assumes the input samples are seperated by row\n",
    "    training_count = round(row_count * split[1] / (split[1] + split[2]))\n",
    "    \n",
    "    validation_set = remaining_set[:training_count]\n",
    "    test_set = remaining_set[training_count:]\n",
    "    \n",
    "    return training_set, validation_set, test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "training, validation, test = get_sets(clean_data, [1/3, 1/3, 1/3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptron (2 Input Linear Unit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class linear_unit():\n",
    "    def __init__(self, num_inputs):\n",
    "        self.weights = np.random.rand(num_inputs + 1)\n",
    "        \n",
    "    def output(self, data_point):\n",
    "        \"\"\"\n",
    "        Returns the linear combination of the input and this unit's weights\n",
    "        \"\"\"\n",
    "        # total = w0*1 + w1x1 + w2x2 + ...\n",
    "        data_with_bias = np.hstack((np.array([1]), data_point[:-1]))\n",
    "        t = data_with_bias * self.weights\n",
    "        t = np.sum(t)\n",
    "        \n",
    "        return t     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class neural_net():\n",
    "    def __init__(self, num_inputs, num_hidden, num_outputs, squash_f, squash_d_f):\n",
    "        self.hidden_units = []\n",
    "        for _ in range(num_hidden):\n",
    "            self.hidden_units.append(linear_unit(num_inputs))\n",
    "        self.output_units = []\n",
    "        for _ in range(num_outputs):\n",
    "            self.output_units.append(linear_unit(num_hidden))\n",
    "        self.squash_fn = squash_f\n",
    "        self.squash_d_fn = squash_d_f    \n",
    "        \n",
    "    def output(self, data_point):\n",
    "        \"\"\"\n",
    "        Returns the array of outputs from the output units\n",
    "        \"\"\"            \n",
    "        \n",
    "        return output_at_layer(data_point, 1)   \n",
    "    \n",
    "    def output_at_layer(self, data_point, layer):\n",
    "        \n",
    "        \"\"\"\n",
    "        Returns the array of outputs from the units on the specified layer\n",
    "        0 - hidden\n",
    "        1 - output\n",
    "        \"\"\"\n",
    "        ## Calculate squashed hidden outputs, with bias\n",
    "        hidden_out = [1]\n",
    "        for unit in self.hidden_units:\n",
    "            hidden_out.append(self.squash_fn(unit.output(data_point)))\n",
    "        \n",
    "        if layer == 0:\n",
    "            return hidden_out\n",
    "            \n",
    "        ## Calculate squashed outputs using hidden outputs\n",
    "        out = []\n",
    "        for unit in self.output_units:\n",
    "            out.append(self.squash_fn(unit.output(hidden_out)))    \n",
    "            \n",
    "        return out\n",
    "        \n",
    "        \n",
    "    def error(self, data):\n",
    "        \"\"\"\n",
    "        Returns the sum squared error of the data using this network's output units\n",
    "        \"\"\"\n",
    "        sum = 0\n",
    "        for d in data:\n",
    "            for out in self.output(d):\n",
    "                o = 1 if out > 0 else -1\n",
    "                sum = sum + (d[2] - o)**2\n",
    "        return sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Squashing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sigmoid function\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Sigmoid derivative function\n",
    "def sigmoid_d(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "# Hyperbolic tan function\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "# Hyperbolic tan derivative function\n",
    "def tanh_d(x):\n",
    "    return 1 - tanh(x)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 3.4893   6.69    -1.2042  -0.38751  1.     ]\n",
      "[0.8967325518619476]\n",
      "8725.838658222196\n"
     ]
    }
   ],
   "source": [
    "net = neural_net(4, 4, 1, sigmoid, sigmoid_d)\n",
    "print(training[0])\n",
    "print(net.output(training[0]))\n",
    "print(net.error(training))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backpropagation(squash_f, squash_d_f, learning_rate, epochs):    \n",
    "    \"\"\"\n",
    "    Find the network with the lowest error based on the number of hidden units after being trained with backpropagation\n",
    "    \"\"\"\n",
    "    best_network = None\n",
    "    for h in range(4, 0, -1):\n",
    "        # Step 1 Initialize network\n",
    "        network = neural_net(4, h, 1, squash_f, squash_d_f)\n",
    "        \n",
    "        # TODO Train network and do the backpropagation\n",
    "        # From NN-MitchelChapter4-2 on canvas\n",
    "        # Alex - Started on it but need to be careful with tanh. The notes on canvas works out sigmoid in detail but not tanh\n",
    "        #           Using a more generalized form of the equations for the algorithm in 1.2\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            for d in training:\n",
    "                d_with_bias = np.hstack((np.array([1]), d[:-1]))\n",
    "                hidden_out = network.output_at_layer(d, 0)\n",
    "                \n",
    "                # Step 2 but replacing specific sigmoid derivative with the squash derivative\n",
    "                delta_out = []\n",
    "                for unit in network.output_units:\n",
    "                    unit_out = unit.output(hidden_out)\n",
    "                    unit_delta = squash_d_f(unit_out) * (d[4] - squash_f(unit_out))\n",
    "                    delta_out.append(unit_delta)\n",
    "\n",
    "                # Step 3 but replacing specific sigmoid derivative with the squash derivative\n",
    "                delta_hidden = []\n",
    "                for unit in network.hidden_units:\n",
    "                    unit_out = unit.output(d_with_bias)\n",
    "                    unit_delta = 0 # TODO squash derivative * sum(weights * delta_out)\n",
    "                    delta_hidden.append(unit_delta)\n",
    "\n",
    "                # Step 4 update weights\n",
    "                # TODO\n",
    "        \n",
    "        # If this network has lower error from validation set, set as best\n",
    "        if best_network is None or network.error(validation) < best_network.error(validation):\n",
    "            best_network = network\n",
    "            \n",
    "    return best_network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (5,) (4,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-50-f5b47bc11373>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# TODO Random learning rate and epochs. I don't know what to use\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mbest_net\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbackpropagation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msigmoid_d\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.01\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m50\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbest_net\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbest_net\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merror\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-49-c39c82dca425>\u001b[0m in \u001b[0;36mbackpropagation\u001b[1;34m(squash_f, squash_d_f, learning_rate, epochs)\u001b[0m\n\u001b[0;32m     18\u001b[0m                 \u001b[0mdelta_out\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0munit\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput_units\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m                     \u001b[0munit_out\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0munit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m                     \u001b[0munit_delta\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msquash_d_f\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0munit_out\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0msquash_f\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0munit_out\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m                     \u001b[0mdelta_out\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0munit_delta\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-45-b6b304c9a399>\u001b[0m in \u001b[0;36moutput\u001b[1;34m(self, data_point)\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[1;31m# total = w0*1 + w1x1 + w2x2 + ...\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0mdata_with_bias\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_point\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m         \u001b[0mt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_with_bias\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m         \u001b[0mt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (5,) (4,) "
     ]
    }
   ],
   "source": [
    "# TODO Random learning rate and epochs. I don't know what to use\n",
    "best_net = backpropagation(sigmoid, sigmoid_d, 0.01, 50)\n",
    "print(training[0])\n",
    "print(best_net.output(training[0]))\n",
    "print(best_net.error(training))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
